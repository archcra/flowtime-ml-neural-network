<h1>
  <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" target="_blank"> Neural networks - Step by step #3 </a></h1>

<h2> The Forward Pass</h2>
$$ net_{h1} = w_1\times x_1 + w_2\times x_2 + b_1 \times 1 = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775 $$
<p>
We then squash it using the logistic function to get the output of h<sub>1</sub>(a<sub>1</sub>):
</p>
$$ a_1 = out_{h1} = \frac{1}{1+ e ^{-net_{h1}}} = \frac{1}{1+ e ^{-0.3775}} = 0.593269992 $$
$$ a_2 = out_{h2} =  0.596884378 $$
$$ net_{o1} = w_5\times out_{h1} + w_6\times out_{h2} + b_2 \times 1 = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967 $$
$$ out_{o1} = \frac{1}{1+ e ^{-net_{o1}}} = \frac{1}{1+ e ^{-1.105905967}} = 0.75136507 $$
$$ out_{o2} =  0.772928465 $$
<h3> Calculating the Total Error </h3>
$$ E_{total} =  \sum \frac{1}{2} (target - output)^2 $$
