<h1>
  <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" target="_blank"> Neural networks - Step by step #4 </a></h1>

<h2> Calculating the Total Error</h2>

$$ E_{total} =  \sum \frac{1}{2} (target - output)^2 $$
$$ E_{o1} =   \frac{1}{2} (target_{o1} - output_{o1})^2 =    \frac{1}{2} ( 0.01 -    0.75136507) = 0.274811083 $$
$$ E_{o2} =  \frac{1}{2} (target_{o2} - output_{o2})^2 =    \frac{1}{2} ( 0.99 -    0.772928465) = 0.23560026 $$
$$ E_{total} =  E_{o1} + E_{o2} = 0.274811083 + 0.23560026 = 0.298371109 $$
<p>
  目标就是一步步（训练或学习）来降低E<sub>total</sub>。通过修改权重(W)来降低E。如何修改W？
</p>
<div style="margin-top: -2em;">
<a href="https://brilliant.org/wiki/backpropagation/"> <strong style="font-size: 2em;  " class="attention"> Backpropagation </strong>
</a>
<pre> short for "backward propagation of errors," is an algorithm for supervised learning of artificial neural networks using gradient descent.
  Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights.
  It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.
The "backwards" part of the name stems from the fact that calculation of the gradient proceeds backwards through the network,
with the gradient of the final layer of weights being calculated first and the gradient of the first layer of weights being calculated last.
</pre>
</div>
