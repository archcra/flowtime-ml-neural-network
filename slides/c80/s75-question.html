

<a href="https://www.geeksforgeeks.org/activation-functions-neural-networks/" target="=_blank"> <h1>为什么activation function不能是线性的？</h1></a>

<pre>
Here,

   z(1) is the vectorized output of layer 1
   W(1) be the vectorized weights assigned to neurons
   of hidden layer i.e. w1, w2, w3 and w4
   X be the vectorized input features i.e. i1 and i2
   b is the vectorized bias assigned to neurons in hidden
   layer i.e. b1 and b2
   a(1) is the vectorized form of any linear function.

(Note: We are not considering activation function here)
```

Layer 2 i.e. output layer :-
```
//  Note : Input for layer
//   2 is output from layer 1
z(2) = W(2)a(1) + b(2)
a(2) = z(2)
```


Calculation at Output layer:

```
// Putting value of z(1) here

z(2) = (W(2) * [W(1)X + b(1)]) + b(2)

z(2) = [W(2) * W(1)] * X + [W(2)*b(1) + b(2)]

Let,
    [W(2) * W(1)] = W
    [W(2)*b(1) + b(2)] = b

Final output : z(2) = W*X + b
Which is again a linear function

```
</pre>
