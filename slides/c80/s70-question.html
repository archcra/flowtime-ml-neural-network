为什么activation function不能是线性的？

https://www.geeksforgeeks.org/activation-functions-neural-networks/



Activation functions in Neural Networks

It is recommended to understand what is a neural network before reading this article. In The process of building a neural network, one of the choices you get to make is what activation function to use in the hidden layer as well as at the output layer of the network. This article discusses some of the choices.

Elements of a Neural Network :-
Input Layer :- This layer accepts input features. It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer.
Hidden Layer :- Nodes of this layer are not exposed to the outer world, they are the part of the abstraction provided by any neural network. Hidden layer performs all sort of computation on the features entered through the input layer and transfer the result to the output layer.
Output Layer :- This layer bring up the information learned by the network to the outer world.

What is an activation function and why to use them?

Definition of activation function:- Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.

Explanation :-
We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function. In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases.


Why do we need Non-linear activation functions :-
A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks.

Mathematical proof :-

Suppose we have a Neural net like this :-

Elements of the diagram :-
Hidden layer i.e. layer 1 :-
```
z(1) = W(1)X + b(1)
a(1) = z(1)
Here,

   z(1) is the vectorized output of layer 1
   W(1) be the vectorized weights assigned to neurons
   of hidden layer i.e. w1, w2, w3 and w4
   X be the vectorized input features i.e. i1 and i2
   b is the vectorized bias assigned to neurons in hidden
   layer i.e. b1 and b2
   a(1) is the vectorized form of any linear function.

(Note: We are not considering activation function here)
```

Layer 2 i.e. output layer :-
```
//  Note : Input for layer
//   2 is output from layer 1
z(2) = W(2)a(1) + b(2)
a(2) = z(2)
```


Calculation at Output layer:

```
// Putting value of z(1) here

z(2) = (W(2) * [W(1)X + b(1)]) + b(2)

z(2) = [W(2) * W(1)] * X + [W(2)*b(1) + b(2)]

Let,
    [W(2) * W(1)] = W
    [W(2)*b(1) + b(2)] = b

Final output : z(2) = W*X + b
Which is again a linear function

```
