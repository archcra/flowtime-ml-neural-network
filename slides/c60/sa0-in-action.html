<h1>名词 </h1>

<pre>
ReLU
Rectified Linear Unit (ReLU)
https://mr-why.com/machinelearning/tom-mitchell-machine-learning-04-dnn-cnn-rnn


Comparing with Sigmoid, the only difference is the activation function *f*. ReLU change the sigmoid function to thresholded output. Note that ReLU is still linear classifier!

Many types of parameterized units

    Sigmoid units
    ReLU
    Leaky ReLU (fixed non-zero slope for input<0)
    Parametric ReLU (trainable slope)
    Max Pool
    Inner Product
    GRU’s
    LSTM’s
    Matrix multiply
    .... no end in sight





Softmax

https://en.wikipedia.org/wiki/Softmax_function


In mathematics, the softmax function, or normalized exponential function,[1]:198 is a generalization of the logistic function that "squashes" a K-dimensional vector z {\displaystyle \mathbf {z} } \mathbf {z} of arbitrary real values to a K-dimensional vector σ ( z ) {\displaystyle \sigma (\mathbf {z} )} \sigma (\mathbf {z} ) of real values in the range (0, 1) that add up to 1.
</pre>
